{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8mZ/4s0VZB3mzXdHjdnj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajaramk8/pyspark_recipes/blob/main/pyspark001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "OXts2pb7e0v0",
        "outputId": "df55c51e-3c97-47e6-ca87-b3ffa6505ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,068 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,342 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,455 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,735 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,784 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 7,617 kB in 1s (5,541 kB/s)\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ba9b4d67d30>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a94922f3b372:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "MHK-hF0Fe7xR",
        "outputId": "121e4c5c-52ab-40ae-aed9-4b7ad7b28c4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ba9b4d67d30>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a94922f3b372:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"001\", \"Monika\", \"Arora\", 100000, \"2014-02-20 09:00:00\", \"HR\"),(\"002\", \"Niharika\", \"Verma\", 300000, \"2014-06-11 09:00:00\", \"Admin\"),(\"003\", \"Vishal\", \"Singhal\", 300000, \"2014-02-20 09:00:00\", \"HR\"),(\"004\", \"Amitabh\", \"Singh\", 500000, \"2014-02-20 09:00:00\", \"Admin\"),(\"005\", \"Vivek\", \"Bhati\", 500000, \"2014-06-11 09:00:00\", \"Admin\")]\n",
        "myschema = [\"workerid\",\"firstname\",\"lastname\",\"salary\",\"joiningdate\",\"depart\"]\n",
        "df = spark.createDataFrame(data,schema=myschema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPV6AlOqe-BW",
        "outputId": "1d6f1996-491a-4385-f7ee-610e27558ddf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+--------+------+-------------------+------+\n",
            "|workerid|firstname|lastname|salary|        joiningdate|depart|\n",
            "+--------+---------+--------+------+-------------------+------+\n",
            "|     001|   Monika|   Arora|100000|2014-02-20 09:00:00|    HR|\n",
            "|     002| Niharika|   Verma|300000|2014-06-11 09:00:00| Admin|\n",
            "|     003|   Vishal| Singhal|300000|2014-02-20 09:00:00|    HR|\n",
            "|     004|  Amitabh|   Singh|500000|2014-02-20 09:00:00| Admin|\n",
            "|     005|    Vivek|   Bhati|500000|2014-06-11 09:00:00| Admin|\n",
            "+--------+---------+--------+------+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"worktab\")\n",
        "spark.sql(\"select a.workerid,a.firstname,a.lastname,a.salary,a.joiningdate,a.depart from worktab a, worktab b where a.salary=b.salary and a.workerid !=b.workerid\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc_OguyDh77L",
        "outputId": "28380881-419e-47f7-b89e-6e91e4d422ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+--------+------+-------------------+------+\n",
            "|workerid|firstname|lastname|salary|        joiningdate|depart|\n",
            "+--------+---------+--------+------+-------------------+------+\n",
            "|     002| Niharika|   Verma|300000|2014-06-11 09:00:00| Admin|\n",
            "|     003|   Vishal| Singhal|300000|2014-02-20 09:00:00|    HR|\n",
            "|     004|  Amitabh|   Singh|500000|2014-02-20 09:00:00| Admin|\n",
            "|     005|    Vivek|   Bhati|500000|2014-06-11 09:00:00| Admin|\n",
            "+--------+---------+--------+------+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "0qGJcdh4iIcU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Through Spark DSL\n",
        "finaldf = df.alias(\"a\").join(df.alias(\"b\"), (col(\"a.salary\") == col(\"b.salary\")) & (col(\"a.workerid\") != col(\"b.workerid\")), \"inner\").select(col(\"a.workerid\"), col(\"a.firstname\"), col(\"a.lastname\"), col(\"a.salary\"), col(\"a.joiningdate\"), col(\"a.depart\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k7sC43uiIOr",
        "outputId": "21aa5578-7133-40de-d30c-3f1aa9ecf8e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+--------+------+-------------------+------+\n",
            "|workerid|firstname|lastname|salary|        joiningdate|depart|\n",
            "+--------+---------+--------+------+-------------------+------+\n",
            "|     002| Niharika|   Verma|300000|2014-06-11 09:00:00| Admin|\n",
            "|     003|   Vishal| Singhal|300000|2014-02-20 09:00:00|    HR|\n",
            "|     004|  Amitabh|   Singh|500000|2014-02-20 09:00:00| Admin|\n",
            "|     005|    Vivek|   Bhati|500000|2014-06-11 09:00:00| Admin|\n",
            "+--------+---------+--------+------+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Need the dates when the status gets changed like ordered to dispatched)"
      ],
      "metadata": {
        "id": "csjfGow5i2if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "      (1, \"1-Jan\", \"Ordered\"),\n",
        "      (1, \"2-Jan\", \"dispatched\"),\n",
        "      (1, \"3-Jan\", \"dispatched\"),\n",
        "      (1, \"4-Jan\", \"Shipped\"),\n",
        "      (1, \"5-Jan\", \"Shipped\"),\n",
        "      (1, \"6-Jan\", \"Delivered\"),\n",
        "      (2, \"1-Jan\", \"Ordered\"),\n",
        "      (2, \"2-Jan\", \"dispatched\"),\n",
        "      (2, \"3-Jan\", \"shipped\")]\n",
        "myschema = [\"orderid\",\"statusdate\",\"status\"]\n",
        "df = spark.createDataFrame(data,schema=myschema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNFbVQYuiwTR",
        "outputId": "6b3c3fd0-588c-4fdf-cb86-7bd4bd706ef3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+\n",
            "|orderid|statusdate|    status|\n",
            "+-------+----------+----------+\n",
            "|      1|     1-Jan|   Ordered|\n",
            "|      1|     2-Jan|dispatched|\n",
            "|      1|     3-Jan|dispatched|\n",
            "|      1|     4-Jan|   Shipped|\n",
            "|      1|     5-Jan|   Shipped|\n",
            "|      1|     6-Jan| Delivered|\n",
            "|      2|     1-Jan|   Ordered|\n",
            "|      2|     2-Jan|dispatched|\n",
            "|      2|     3-Jan|   shipped|\n",
            "+-------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"ordertab\")\n",
        "spark.sql(\"select * \\\n",
        "               from ordertab \\\n",
        "               where status = 'dispatched' \\\n",
        "               and orderid in(\\\n",
        "                   select orderid \\\n",
        "                   from ordertab \\\n",
        "                   where status = 'Ordered')\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utWfaCq2jrRh",
        "outputId": "8a36c23d-6147-4d1c-ce86-73b8f1748591"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+\n",
            "|orderid|statusdate|    status|\n",
            "+-------+----------+----------+\n",
            "|      1|     2-Jan|dispatched|\n",
            "|      1|     3-Jan|dispatched|\n",
            "|      2|     2-Jan|dispatched|\n",
            "+-------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Through DSL\n",
        "result = df.filter(\n",
        "    (col(\"status\") == \"dispatched\") &\n",
        "    (col(\"orderid\").isin(\n",
        "        *[row[0] for row in df.filter(col(\"status\") == \"Ordered\").select(\"orderid\").collect()]\n",
        "    ))\n",
        ")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWcyzWnSnTHD",
        "outputId": "9738ffc9-f47e-490b-a725-f91a32568f89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+\n",
            "|orderid|statusdate|    status|\n",
            "+-------+----------+----------+\n",
            "|      1|     2-Jan|dispatched|\n",
            "|      1|     3-Jan|dispatched|\n",
            "|      2|     2-Jan|dispatched|\n",
            "+-------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How much did the value in a column increase?"
      ],
      "metadata": {
        "id": "HjKAyk6anVY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1111, \"2021-01-15\", 10),\n",
        "        (1111, \"2021-01-16\", 15),\n",
        "        (1111, \"2021-01-17\", 30),\n",
        "        (1112, \"2021-01-15\", 10),\n",
        "        (1112, \"2021-01-15\", 20),\n",
        "        (1112, \"2021-01-15\", 30)]\n",
        "\n",
        "myschema = [\"sensorid\", \"timestamp\", \"values\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=myschema)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihm7ByVTn0Z1",
        "outputId": "f8d66df2-2189-4198-e09b-156e93697559"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|sensorid| timestamp|values|\n",
            "+--------+----------+------+\n",
            "|    1111|2021-01-15|    10|\n",
            "|    1111|2021-01-16|    15|\n",
            "|    1111|2021-01-17|    30|\n",
            "|    1112|2021-01-15|    10|\n",
            "|    1112|2021-01-15|    20|\n",
            "|    1112|2021-01-15|    30|\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import *"
      ],
      "metadata": {
        "id": "DK0weJOuoJW4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "8SQCM4aBCAbw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = Window.partitionBy(\"sensorid\").orderBy(\"values\")"
      ],
      "metadata": {
        "id": "feYOxgOWn9zi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1"
      ],
      "metadata": {
        "id": "fMDGoeS6oMui",
        "outputId": "fb50eb04-1a1b-490e-f848-a4f7e4924534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.window.WindowSpec at 0x7ba991b4d330>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waSTA172oOgh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finaldf = df.withColumn(\"nextvalues\", lead(\"values\", 1).over(d1)) \\\n",
        "    .filter(col(\"nextvalues\").isNotNull()) \\\n",
        "    .withColumn(\"values\", expr(\"nextvalues-values\")) \\\n",
        "    .drop(\"nextvalues\") \\\n",
        "    .orderBy(col(\"sensorid\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qPm8-BPArPy",
        "outputId": "153dc73a-7fb7-467a-c680-beebf56cee7d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|sensorid| timestamp|values|\n",
            "+--------+----------+------+\n",
            "|    1111|2021-01-15|     5|\n",
            "|    1111|2021-01-16|    15|\n",
            "|    1112|2021-01-15|    10|\n",
            "|    1112|2021-01-15|    10|\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Window Function Examples\n"
      ],
      "metadata": {
        "id": "u1wn_CCJCIL_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_VMq6a0yCFgG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}